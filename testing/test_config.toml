# Конфигурация RAG-системы v4.0

# ============================================================================
# Ollama настройки
# ============================================================================
[ollama]
# Базовый URL для Ollama API
base_url = "http://localhost:11434"
# base_url = "http://10.0.6.3:11434"

# Модель для текстовых эмбеддингов
text_embedding_model = "nomic-embed-text:v1.5"

# Модель Vision-Language Model для анализа изображений (OCR + описание)
vlm_model = "qwen3-vl:4b-instruct"

# Модель для Query Expansion (генерации поисковых запросов)
text_generation_model = "qwen3:4b-instruct-2507-q4_K_M"

# Таймаут для запросов к Ollama (в секундах)
timeout = 180.0


# ============================================================================
# Хранилище данных
# ============================================================================
[storage]
# Путь для хранения обработанных изображений
storage_path = "./testing/rag_engine"


# ============================================================================
# Параметры chunking (разбиения текста)
# ============================================================================
[chunking]
# Размер текстового чанка (в символах)
# Рекомендуется: 800-1200 для технической документации
chunk_size = 1024

# Перекрытие между чанками (в символах)
# Рекомендуется: 10-15% от chunk_size
chunk_overlap = 128


# ============================================================================
# Параметры поиска
# ============================================================================
[search]
# Максимальное количество финальных результатов (после реранкинга)
max_results = 5

# Количество кандидатов для гибридного поиска (до реранкинга)
# Рекомендуется: 40-60 для баланса скорости и качества
retrieval_top_k = 50

# Кэширование BM25 индекса на диск
# true = быстрая загрузка при перезапуске, экономия CPU
# false = всегда строит индекс с нуля
bm25_cache_enabled = true

# Автоматически выгружать BM25 из памяти после использования
# true = экономия памяти (~10-50MB), но медленнее при частых запросах
# false = всегда в памяти (рекомендуется)
bm25_auto_unload = false


# ============================================================================
# Query Expansion (расширение запросов через LLM)
# ============================================================================
[query_expansion]
# Включить/выключить расширение запросов
enabled = true

# Максимальное количество поисковых фраз из одного запроса
max_queries = 3


# ============================================================================
# Reranker (Cross-Encoder)
# ============================================================================
[reranker]
# Включить/выключить реранкинг результатов
enabled = true

# Модель Cross-Encoder для реранкинга
# Варианты:
# - BAAI/bge-reranker-v2-m3 (multilingual, best quality, 568MB)
# - cross-encoder/ms-marco-MiniLM-L-12-v2 (English, lighter, 120MB)
model_name = "BAAI/bge-reranker-v2-m3"

# Автоматически выгружать модель из памяти после использования
# true = модель загружается только на время реранкинга
# false = модель остается в памяти (быстрее при частых запросах)
auto_unload = true


# ============================================================================
# Логирование
# ============================================================================
[logging]
# Уровень логирования: DEBUG, INFO, WARNING, ERROR, CRITICAL
level = "INFO"

# Использовать цветной вывод в консоль
use_colors = true

# Использовать эмодзи в логах
use_emoji = false

# Ширина поля имени модуля (символов)
# Рекомендуется: 25-30 для читаемости
module_width = 25

# Путь к файлу для сохранения логов (опционально, закомментируйте для отключения)
# log_file = "./logs/rag_system.log"
