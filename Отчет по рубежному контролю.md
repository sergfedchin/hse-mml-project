# Технический отчет по промежуточному этапу проекта

## Введение

Цель проекта - разработать мультимодальную RAG-систему для поиска по продуктовой документации MDM на русском языке. Система должна уметь работать не только с текстом, но и с таблицами и скриншотами интерфейса, чтобы максимально полно использовать имеющиеся материалы документации. На текущем этапе реализована вся архитектура RAG, а также сервис для доступа к нему.

## Исходные данные и общий подход

В качестве источника данных используется продуктовая документация Universe Data MDM 6.13, подготовленная в формате ReStructuredText (RST) с помощью Sphinx. Документация включает текстовые страницы, таблицы и большое количество скриншотов интерфейса. Для удобства работы и лучшего понимания текста LLM выбрана стратегия предварительной конвертации RST в Markdown.

Основная идея системы состоит в том, чтобы построить поисковый движок, который объединяет векторный поиск по эмбеддингам, классический BM25 поиск и дополнительную обработку изображений через Vision-Language модель. Система реализована как набор модульных компонентов, объединенных в единый класс RAGSystem.

## Обработка и подготовка документации

### Конвертация RST в Markdown

На первом этапе был реализован скрипт для массовой конвертации RST страниц в Markdown. Для этого используется библиотека `pypandoc`. Скрипт проходит по всем RST-файлам в директории документации, конвертирует каждый файл в формат GitHub Flavored Markdown и сохраняет структуру каталогов. Это позволяет работать далее с более простым и удобным для LLM форматом.

### Работа с таблицами в RST

В исходной документации используется сложная разметка RST-таблиц с объединенными ячейками по горизонтали и вертикали. Для их обработки реализован отдельный модуль RSTTableParser, который:
- строит doctree с помощью `docutils`
- находит узлы таблиц
- аккуратно распаковывает структуру с учетом rowspan и colspan
- преобразует таблицы в более простой табличный формат
- конвертирует таблицы в Markdown

### Разбиение на чанки

Текстовая часть документации разбивается на чанки с помощью `MarkdownTextSplitter`. Параметры разбиения задаются в конфигурации:

- размер чанка по символам (по умолчанию 1024)
- перекрытие чанков (по умолчанию 128)

Разбиение учитывает структуру Markdown, что позволяет чаще получать смысловые фрагменты, а не произвольные куски текста.

Для таблиц реализована отдельная логика чанкования. Большие таблицы могут разбиваться на несколько чанков, чтобы не превышать ограничение по размеру. При этом сохраняется информация о заголовках и контексте таблицы.

### Обработка изображений

Для скриншотов интерфейса реализован пайплайн анализа изображений через Vision-Language модель, развернутую локально через Ollama. Изображение конвертируется в base64, после чего модель должна вернуть JSON с двумя полями:

- `ocr_text` - распознанный текст на изображении
- `ui_description` - техническое текстовое описание интерфейса

Ответ валидируется через `pydantic` модель. В случае ошибок парсинга используется `json_repair`. 

### Метаданные и контекст

Для каждого чанка формируются метаданные, которые включают:

- идентификатор документа
- путь к исходному файлу
- URL страницы в опубликованной документации на официальном [сайте](https://doc.ru.universe-data.ru/6.13.0-EE/).
- при необходимости breadcrumbs или другие элементы структуры

Это позволяет восстанавливать контекст результатов поиска и показывать пользователю ссылки на оригинальные страницы.

## Архитектура RAG-системы

Система организована как набор модулей в пакете `src.rag` и объединена фасадным классом `RAGSystem`. Логически можно выделить следующие компоненты:

- `DocumentProcessor` - отвечает за парсинг RST, обработку таблиц, анализ изображений и формирование чанков
- `VectorStorage` - отвечает за хранение эмбеддингов и взаимодействие с векторной базой данных
- `KeywordSearchEngine` - реализует классический BM25 поиск по корпусу
- `HybridRetriever` - объединяет результаты векторного и keyword поиска
- `QueryProcessor` - занимается предобработкой пользовательских запросов и query expansion
- `CrossEncoderRanker` - реализует финальное переранжирование результатов с помощью cross-encoder модели

Внешняя граница системы - это класс `RAGSystem`, который связывает все компоненты вместе и предоставляет высокоуровневые методы:

- `initialize` - инициализация хранилищ и моделей
- `add_document` - добавление нового документа в индекс
- `delete_document` - удаление документа
- `search` - выполнение пользовательского поиска
- `cleanup` - освобождение ресурсов и корректное завершение работы

## Реализация основных компонентов

### Конфигурация и управление параметрами

Конфигурация вынесена в отдельный модуль `config.py` и файл `config.toml`. В коде описан класс `Config`, который:

- читает параметры из TOML файла
- валидирует значения (например, соотношение `chunk_size` и `chunk_overlap`, диапазон `top_k`)
- создает необходимые директории для векторной базы и хранилища изображений
- предоставляет удобный интерфейс для доступа к параметрам (модели, путям, настройкам логирования)

Параметры конфигурации включают:

- адрес и модели для взаимодействия с LLM и VLM
- пути к векторной базе и хранилищу изображений
- параметры разбиения на чанки
- настройки поиска и reranking
- уровень и формат логирования

### Векторное хранилище

В качестве векторной базы используется ChromaDB. `VectorStorage` инкапсулирует работу с коллекциями, добавление и удаление документов, поиск по эмбеддингам и обновление индексов. Эмбеддинги генерируются через Ollama с выбранной моделью эмбеддингов.

### Keyword поиск (BM25)

Для keyword поиска используется библиотека `rank-bm25`. В модуле KeywordSearchEngine реализовано:

- построение корпуса документов для BM25
- кэширование индекса на диск, чтобы не пересчитывать его при каждом запуске
- возможность отключать и выгружать индекс из памяти для экономии ресурсов

BM25 используется как дополнительный источник кандидатов к векторному поиску. Это особенно полезно для коротких запросов и случаев, когда точная формулировка запроса совпадает с текстом документации.

### Query expansion

`QueryProcessor` реализует расширение запросов через LLM (тоже через Ollama). На вход подается исходный запрос пользователя, на выходе получается несколько запросов, извлекающих необходимые элементы, по которым нужно выполнить поиск. Это позволяет повысить точность поиска, так как запросы могут быть длинными и содержать разные термины.

Максимальное количество дополнительных запросов задается в конфигурации (по умолчанию 3). При необходимости модуль можно отключить.

### Гибридный поиск и reranking

`HybridRetriever` получает на вход один или несколько текстовых запросов (после query expansion) и:

- выполняет векторный поиск по эмбеддингам
- выполняет BM25 поиск по тексту
- объединяет и нормализует результаты
- отбирает `top_k` кандидатов для дальнейшего reranking

CrossEncoderRanker затем выполняет reranking кандидатов с помощью cross-encoder модели. Модель получает на вход пары (запрос, текст чанка) и возвращает релевантность. После этого результаты сортируются по оценке, и пользователю возвращается ограниченное количество лучших документов.

## API и тестирование

Поверх `RAGSystem` реализовано HTTP API на FastAPI. Основные эндпоинты:

- POST `/documents` - добавление документа по содержимому RST и связанной информации
- POST `/search` - поиск по текстовому запросу
- DELETE `/documents/{file_id}` - удаление документа из индекса

Для ручного тестирования и отладки реализованы скрипты:

- `test_ragv4.py` - асинхронный скрипт, работающий с RAGSystem напрямую
- `test_ragv4_1.py` - скрипт, отправляющий запросы к HTTP API через requests

Скрипты позволяют:

- добавить один или несколько документов из локальной копии документации
- выполнить серию тестовых запросов
- вывести результаты поиска в удобном для анализа виде
- удалить добавленные документы и очистить ресурсы

## Текущее состояние и ограничения

На текущий момент:

- реализован полный пайплайн обработки текстов, таблиц и изображений
- настроено векторное и keyword хранилище
- функционирует гибридный поиск с reranking
- есть рабочее HTTP API и скрипты тестирования

Основное ограничение на этом этапе - отсутствие систематической оценки качества поиска на специально подготовленном наборе запросов. Пока оценка носит в основном качественный характер через ручные запросы к конкретным страницам документации.

## Планируемые дальнейшие шаги

В оставшееся время планируется:

1. Сгенерировать тестовый датасет запросов по чанкам.
   - Для каждого чанка или группы чанков получить один или несколько вопросов, ответы на которые можно найти именно в этих фрагментах.
   - Постараться покрыть разные типы информации: описания сущностей, поля интерфейса, параметры, таблицы, пошаговые инструкции.

2. Реализовать скрипт автоматической оценки retrieval части.
   - Для каждого вопроса запускать текущий RAG-пайплайн.
   - Проверять, попали ли релевантные чанки в топ-k результатов.
   - Считать метрики вроде Hit@k и MRR, а также оценивать контекстную релевантность.

3. Реализовать и протестировать упрощенный baseline.
   - Вариант с одной модальностью
   - Вариант без реранкинга и query expansion

4. При необходимости донастроить параметры системы.
   - Подобрать оптимальный размер чанков и перекрытие.
   - Подкорректировать количество кандидатов на этапе retrieval.
   - Пересмотреть промпты для query expansion и анализа изображений, если это даст прирост качества.
